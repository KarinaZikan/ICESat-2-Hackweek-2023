{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-union",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Snow Depth - ICESat-2 Applications Tutorial\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**This is a placeholder Notebook for the snow depth tutorial**\n",
    "- gain experience in working with SlideRule to access and pre-process ICESat-2 data\n",
    "- learn how use projections and interpolation to compare ICESat-2 track data with gridded raster products\n",
    "- develop a general understanding of how to measure snow depths with LiDAR, and learn about opportunities and challenges when using ICEsat-2 along-track products\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d863c-5ad1-4b9f-9fa0-416ff04396a4",
   "metadata": {},
   "source": [
    "## Computing environment\n",
    "\n",
    "We'll be using the following open source Python libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5720d5-d805-4c24-a92e-a2238805cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet\n",
    "from ipyleaflet import Map, Rectangle, basemaps, basemap_to_tiles, TileLayer, SplitMapControl, Polygon\n",
    "\n",
    "import ipywidgets\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# maybe the ones below??\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import CRS\n",
    "from sliderule import sliderule, icesat2, earthdata, h5\n",
    "from scipy.interpolate import RectBivariateSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb53c4b-34b2-4a51-a1d1-ec317a9f27e2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Draft outline:\n",
    "\n",
    "Background:\n",
    "- How do we measure snow depth with lidar\n",
    "- How do we measure snow depth with ICESat-2 specifically\n",
    "- What challenges do we need to think about\n",
    "    - Comparing a raster to an IS-2 transect\n",
    "    - Geolocation\n",
    "    - Vegetation, slope affects\n",
    "- What do we need\n",
    "    - A region of interest\n",
    "    - ICESat-2 data\n",
    "    - A snow-free reference DEM\n",
    "        \n",
    "Define ROI:\n",
    "- Define bounding box\n",
    "- Plot box over satellite image? Map showing location of ROI?\n",
    "    \n",
    "Calculate ATL06:\n",
    "- Talk a little about why were using ALT06 (presuming we decide to use ATL06)\n",
    "- Set Parameters\n",
    "    - Talk a little about the parameters, provide link to more detailed info from SlideRule website\n",
    "- Show code cell to calculate ICESat-2 data for demonstration\n",
    "- Read in previously calculated data\n",
    "- Plot ICESat-2 tracks on ROI, height colorbar?\n",
    "    \n",
    "Pull DEM:\n",
    "- Talk about raster data?\n",
    "- Show code cell to pull 3DEP data for demonstration purposes\n",
    "- Read in previously downloaded DEM\n",
    "- Plot\n",
    "    \n",
    "Compare IS-2 and DEM:\n",
    "\t- Remind audience that we need to process the raster data to be comparable to a point transect\n",
    "\t- Integrate DEM (mention that there are a number of ways to do this, we are showing a simple way for time)\n",
    "\t- Plot snow depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73344a2-f797-4218-a99f-a9db0338f002",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Data\n",
    "\n",
    "Use SlideRule to acquire ATL06 data as well as the gridded data, noting customization for averaging footprint, photon identification: this will be a modified ATL06 download that includes some vegetation filtering\n",
    "\n",
    "\n",
    "**Data considerations:**\n",
    "\n",
    "pre-download the demonstration files to the CryoHub checking in on maximum file size - try to keep the region of interest relatively small for the tutorial? @zachghiaccio to contact CryoHub folks\n",
    "checking to see if SlideRule can be used to acquire the gridded DEM @KarinaZikan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38c7c2-e856-444d-9b74-d517a53213c7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "To obtain snow depth with ICESat-2, we are going to use SlideRule for its high level of customization. We are going to look at snow depth data over the Arctic Coastal Plain (ACP) of Alaska, which is a relatively flat region with little vegetation. Thus, we should expect good agreement between ICESat-2 and our lidar rasters of interest.\n",
    "\n",
    "After we initialize SlideRule, we define our region of interest. Notice that there are two options in the cell below. This is because SlideRule accepts either the coordinates of a box/polygon or a geoJSON for the `region` input. \n",
    "\n",
    "We are going to use the `sliderule.icesat2.toregion()` function in this tutorial using a pre-generated polygon, but we have the secondary method included for your personal reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198d7df-b3d2-476e-b7f3-1169c2e9c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SlideRule\n",
    "icesat2.init(\"slideruleearth.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3065f-2a34-41e6-a845-604a3d1e0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region of interest over ACP, Alaska\n",
    "region = [ {\"lon\":-148.85, \"lat\": 69.985},\n",
    "           {\"lon\":-148.527, \"lat\": 69.985},\n",
    "           {\"lon\":-148.527, \"lat\": 70.111},\n",
    "           {\"lon\":-148.85, \"lat\": 70.111},\n",
    "           {\"lon\":-148.85, \"lat\": 69.985} ]\n",
    "\n",
    "# Alternate method, with geoJSON [TO BE ADDED]\n",
    "#path = '/home/jovyan/ICESat-2-Hackweek-2023/book/tutorials/applications-snow-depth/'\n",
    "#region = icesat2.toregion(f'{path}acp_lidar_box.geojson')[\"poly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf708a-f54a-4cd4-aa12-af46cefbdc36",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now we are going to build our SlideRule request with a few more parameters. \n",
    "\n",
    "**Review**: In the below cell, what does each parameter represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160603f1-902d-47a4-af5e-3598c61a3d9b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Keep this cell hidden until activity is over**\n",
    "\n",
    "We defined our `region` above, so let's run through the remaining parameters:\n",
    "* `srt`: Only land photons will be considered.\n",
    "* `cnf`: Only high-confidence photons.\n",
    "* `atl08_class`: Only ground photons, as identified by the ATL08 algorithm.\n",
    "* `ats`: The maximum along-track spread (uncertainty) in aggregated photons will be 5 m.\n",
    "* `len`: The length of each segment of aggregated photons will be 20 m.\n",
    "* `res`: The along-track resolution will be 10 m. Because each segment will be 20 m long, there will be overlap between successive data points.\n",
    "* `maxi`: The SlideRule refinement algorithm will iterate 5 times per segment at maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ae02f-1e4c-4eb3-b350-14cb881179f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SlideRule request\n",
    "parms = {\n",
    "    \"poly\": region,\n",
    "    \"srt\": icesat2.SRT_LAND,\n",
    "    \"cnf\": icesat2.CNF_SURFACE_HIGH,\n",
    "    \"atl08_class\": [\"atl08_ground\"],\n",
    "    \"ats\": 5.0,\n",
    "    \"len\": 20.0,\n",
    "    \"res\": 10.0,\n",
    "    \"maxi\": 5\n",
    "}\n",
    "\n",
    "is2_df = icesat2.atl06p(parms, \"nsidc-s3\")\n",
    "\n",
    "is2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1a042-89f5-4792-ba0c-b1df3c312678",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "You probably noticed that the algorithm took a long time to generate the GeoDataFrame. That is because (i) our region of interest was rather large and (ii) we obtained all ICESat-2 tracks in the ROI since its launch (2018).\n",
    "\n",
    "Just for the sake of interest, let's take a look at all of the ICESat-2 tracks over ACP, Alaska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37a8fa-87b8-4dc2-8e04-a79f87f6e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample plot for all of the ICESat-2 tracks since its launch\n",
    "\n",
    "# Calculate Extent\n",
    "lons = [p[\"lon\"] for p in region]\n",
    "lats = [p[\"lat\"] for p in region]\n",
    "lon_margin = (max(lons) - min(lons)) * 0.1\n",
    "lat_margin = (max(lats) - min(lats)) * 0.1\n",
    "\n",
    "# Create Plot\n",
    "fig,(ax1,ax2) = plt.subplots(num=None, ncols=2, figsize=(12, 6))\n",
    "box_lon = [e[\"lon\"] for e in region]\n",
    "box_lat = [e[\"lat\"] for e in region]\n",
    "\n",
    "# Plot SlideRule Ground Tracks\n",
    "ax1.set_title(\"SlideRule Zoomed Ground Tracks\")\n",
    "is2_df.plot(ax=ax1, column=is2_df[\"h_mean\"], cmap='winter_r', s=1.0, zorder=3)\n",
    "ax1.plot(box_lon, box_lat, linewidth=1.5, color='r', zorder=2)\n",
    "ax1.set_xlim(min(lons) - lon_margin, max(lons) + lon_margin)\n",
    "ax1.set_ylim(min(lats) - lat_margin, max(lats) + lat_margin)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot SlideRule Global View\n",
    "ax2.set_title(\"SlideRule Global Reference\")\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.plot(ax=ax2, color='0.8', edgecolor='black')\n",
    "ax2.plot(box_lon, box_lat, linewidth=1.5, color='r', zorder=2)\n",
    "ax2.set_xlim(min(lons)-0.25, max(lons)+0.25)\n",
    "ax2.set_ylim(min(lats)-0.25), max(lats)-0.25)\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Show Plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ef5f0-b0ad-41e6-b969-5bd6d357b614",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "It is cool to see all of the available data, but we only have lidar DEMs available from March 2022. So, we are going to subset the data to include one ICESat-2 track (**RGT 1097**) in **March 2022**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babc890-f9b6-4e44-a105-0383c554ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset ICESat-2 data to single RGT, time of year\n",
    "is2_df_subset = is2_df[is2_df['rgt']==1097]\n",
    "is2_df_subset = is2_df_subset.loc['2022-03']\n",
    "\n",
    "is2_df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd227bb3-0063-4bf1-a999-53569d222a3b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## sample the DTM to ICESat-2 ground track \n",
    "mention importance of geolocation errors and choice of interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df5fff-ef70-46ba-92a6-dd5c663a042d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Our ICESat-2 data is ready to go! Now it's time to load the airborne lidar data, and co-register it with ICESat-2.\n",
    "\n",
    "The lidar data we will use is from the University of Alaska, Fairbanks (UAF). The UAF lidar obtains snow-on and snow-off DEMs/DTMs with a 1064 nm (near-infrared) laser, from which it can also derive snow depth. \n",
    "\n",
    "UAF lidar rasters normally have a spatial resolution of 0.5 m, which would take a long time to process for this tutorial. As a compromise between computation speed and resolution, we will use rasters that have been coarsened to 3 m resolution.\n",
    "\n",
    "The best way to load lidar DEMs/DTMs is to use `rioxarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22049dea-1657-4ca4-8daa-af7ca056ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for UAF rasters\n",
    "f_snow_off = f'{path}/lidar-dems/coastalplain_2022aug31_dtm_3m.tif'\n",
    "f_snow_on = f'{path}/lidar-dems/coastalplain_2022mar12_snowdepth_3m.tif'\n",
    "\n",
    "# Load files as rioxarray datasets\n",
    "lidar_snow_off = rioxarray.open_rasterio(f_snow_off)\n",
    "lidar_snow_on = rioxarray.open_rasterio(f_snow_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751141c-8598-4adc-baf6-43e49cd830aa",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "It is not immediately obvious, but the UAF rasters are in a different spatial projection than ICESat-2. UAF is in **EPSG:32606**, ICESat-2 is in **WGS84/EPSG:4326**.\n",
    "\n",
    "In order to directly compare these two datasets, we are going to add reprojected coordinates to the ICESat-2 DataFrame. In essence, we will go from **latitude/longitude** to **northing/easting**. Luckily, there is an easy way to do this with GeoPandas, specifically the `geopandas.to_crs()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32511c5d-0e06-441c-acfa-3a88bd09094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ICESat-2 coordinate projection\n",
    "is2_df = is2_df.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Change to EPSG:32606\n",
    "is2_df = is2_df.to_crs(\"EPSG:32606\")\n",
    "\n",
    "is2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2f79c-d864-400b-a3bf-deb8d0d66dac",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now, we are going to co-register both rasters to the queried ICESat-2 data. The function below is fairly long, but the gist is that we are using a spline interpolant to match both the snow-off UAF data (surface height) and UAF snow depths with ICESat-2 surface heights. The resulting DataFrame will have both ICESat-2 and UAF data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afca8b56-0a0d-41f3-95ab-f8a64f78af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coregister_is2(lidar_snow_off, lidar_snow_on, is2_df):\n",
    "    \"\"\"\n",
    "    Co-registers UAF data with ICESat-2 data with a rectangular bivariate\n",
    "    spline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lidar_height : rioxarray dataset\n",
    "        Lidar DEM/DTM in rioarray format.\n",
    "    lidar_snow_depth : rioxarray dataset\n",
    "        Lidar-derived snow depth in rioxarray format.\n",
    "    is2_df : GeoDataFrame\n",
    "        GeoDataFrame for the ICESat-2 data generated with SlideRule.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is2_uaf_df : GeoDataFrame\n",
    "        Contains the coordinate and elevation data that matches best with\n",
    "        ICESat-2.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define x/y coordinates from snow-off data\n",
    "    x0 = np.array(lidar_snow_off.x)\n",
    "    y0 = np.array(lidar_snow_off.y)\n",
    "    \n",
    "    # Do the same, but for snow depth data\n",
    "    xs = np.array(lidar_snow_on.x)\n",
    "    ys = np.array(lidar_snow_on.y)\n",
    "    \n",
    "    # Filter NaNs that would otherwise mess up the interpolators\n",
    "    dem_heights = np.array(lidar_height.sel(band=1))[::-1,:]\n",
    "    dem_heights[np.isnan(dem_heights)] = -9999\n",
    "    dem_depths = np.array(lidar_snow_depth.sel(band=1))[::-1,:]\n",
    "    dem_depths[np.isnan(dem_depths)] = -9999\n",
    "    \n",
    "    # Generate interpolators\n",
    "    interp_height = RectBivariateSpline(np.array(y0)[::-1], \n",
    "                                       np.array(x0),\n",
    "                                       dem_heights)\n",
    "    interp_depth = RectBivariateSpline(np.array(ys)[::-1],\n",
    "                                       np.array(x0),\n",
    "                                       dem_depths)\n",
    "    \n",
    "    # Use constructed interpolators to align UAF with ICESat-2. This aligns with all six\n",
    "    # ICESat-2 beams\n",
    "    is2_uaf_df = gpd.GeoDataFrame()\n",
    "    for beam in np.unique(is2_df['gt']):\n",
    "        is2_tmp = is2_pd.loc[is2_pd['gt']==spot]\n",
    "            \n",
    "        # ICESat-2 x/y coordinates\n",
    "        xn = is2_tmp.geometry.x\n",
    "        yn = is2_tmp.geometry.y\n",
    "            \n",
    "        # Define indices within x/y bounds of DEM\n",
    "        i1 = (xn>np.min(x0)) & (xn<np.max(x0))\n",
    "        i1 &= (yn>np.min(y0)) & (yn<np.max(y0))\n",
    "        \n",
    "        lidar_height = interp_height(yn[i1], xn[i1], grid=False)\n",
    "        lidar_snow_depth = interp_depth(yn[i1], xn[i1], grid=False)\n",
    "        # Set x/y coordinates, UAF height/snow depth, and ICESat-2 heights into same DataFrame\n",
    "        tmp = gpd.GeoDataFrame(data={'x': xn[i1],\n",
    "                                     'y': yn[i1],\n",
    "                                     'time': is2_tmp['time'][i1],\n",
    "                                     'beam': is2_tmp['gt'][i1],\n",
    "                                     'lidar_height': lidar_height,\n",
    "                                     'lidar_snow_depth': lidar_snow_depth,\n",
    "                                     'is2_height': is2_tmp['h_mean'][i1],\n",
    "                                     'h_sigma': is2_tmp['h_sigma'][i1],\n",
    "                                     'dh_fit_dx': is2_tmp['dh_fit_dx'][i1]})\n",
    "        \n",
    "        # Concatenate coregistered data to DataFrame\n",
    "        is2_uaf_df = gpd.concat([is2_uaf_df, tmp])\n",
    "        \n",
    "    return is2_uaf_df\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc74fb-e3e3-40fd-ae42-816c622a91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-register ICESat-2 and UAF using the above function\n",
    "is2_uaf_df = coregister_is2(lidar_snow_off, lidar_snow_on, is2_df)\n",
    "\n",
    "is2_uaf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c47de-4159-4527-a1e2-72613faebf9b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "As you can see, we now have a DataFrame that includes several useful variables:\n",
    "* `beam`: ICESat-2 beam (gt1l, gt2l, etc.)\n",
    "* `lidar_height`: Snow-off surface height from UAF lidar.\n",
    "* `lidar_snow_depth`: Snow depth derived from UAF.\n",
    "* `is2_height`: ICESat-2 surface height (snow-on, in this case).\n",
    "* `h_sigma`: ICESat-2 height uncertainty.\n",
    "* `dh_fit_dx`: Along-track slope of the terrain.\n",
    "\n",
    "With this DataFrame, it will now be very simple to derive snow depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bb16c-11e0-4ff1-9928-fb50a7f15293",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## differencing to get a snowdepth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccebeb-99e9-455d-8ca6-b1d0de990ed2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cec5b00-b5e3-49aa-be49-0edf13bee2e5",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## visualize the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99d916-deb8-4af0-9fce-1e38e5c18d87",
   "metadata": {},
   "source": [
    "## Interactive visualization\n",
    "\n",
    "The ipyleaflet library allows us to create interactive map visualizations. Below we define a geographic bounding box for our area of interest, and plot it on an interactive \"slippy map\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-108.3, 39.2, -107.8, 38.8]\n",
    "west, north, east, south = bbox\n",
    "bbox_ctr = [0.5*(north+south), 0.5*(west+east)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-wildlife",
   "metadata": {},
   "source": [
    "Display the bounding box on an interactive basemap for context. All the available basemaps can be found in the [ipyleaflet documentation](https://ipyleaflet.readthedocs.io/en/latest/api_reference/basemaps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(center=bbox_ctr, zoom=10)\n",
    "rectangle = Rectangle(bounds=((south, west), (north, east))) #SW and NE corners of the rectangle (lat, lon)\n",
    "m.add_layer(rectangle)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-watch",
   "metadata": {},
   "source": [
    "### NASA GIBS basemap\n",
    "\n",
    "NASA's [Global Imagery Browse Services (GIBS)](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/gibs) is a great Web Map Tile Service (WMTS) to visualize NASA data as pre-rendered tiled raster images. The NASA [Worldview](https://worldview.earthdata.nasa.gov) web application is a way to explore all GIBS datasets. We can also use ipyleaflet to explore GIBS datasets, like MODIS truecolor images, within a Jupyter Notebook. Use the slider in the image below to reveal the image from 2019-04-25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(center=bbox_ctr, zoom=6)\n",
    "\n",
    "right_layer = basemap_to_tiles(basemaps.NASAGIBS.ModisTerraTrueColorCR, \"2019-04-25\")\n",
    "left_layer = TileLayer()\n",
    "control = SplitMapControl(left_layer=left_layer, right_layer=right_layer)\n",
    "m.add_control(control)\n",
    "\n",
    "m.add_layer(rectangle)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb26cf-5651-4424-bab0-89bdc0b69ff1",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Re-create the map above using different tile layers for both the right and left columns. Label a single point of interest with a marker, such as the city of Grand Junction, Colorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccf31e-1919-45c2-9796-26f7bac25520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution for exercise 1 here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-amateur",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    " 🎉 Congratulations! You've completely this tutorial and have seen how we can add  notebook can be formatted, and how to create interactive map visualization with ipyleaflet.\n",
    " \n",
    "\n",
    "```{note}\n",
    "You may have noticed Jupyter Book adds some extra formatting features that do not necessarily render as you might expect when *executing* a noteook in Jupyter Lab. This \"admonition\" note is one such example.\n",
    "```\n",
    "\n",
    ":::{warning}\n",
    "Jupyter Book is very particular about [Markdown header ordering](https://jupyterbook.org/structure/sections-headers.html?highlight=headers#how-headers-and-sections-map-onto-to-book-structure) to automatically create table of contents on the website. In this tutorial we are careful to use a single main header (#) and sequential subheaders (#, ##, ###, etc.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f5c79-19d7-48ac-9421-852c3c83cc5f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "To further explore the topics of this tutorial see the following detailed documentation:\n",
    "\n",
    "* [Jupyter Book rendering of .ipynb notebooks](https://jupyterbook.org/file-types/notebooks.html)\n",
    "* [Jupyter Book guide on writing narrative content](https://jupyterbook.org/content/index.html)\n",
    "* [ipyleaflet documentation](https://ipyleaflet.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
